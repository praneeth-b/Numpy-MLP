{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Praneeth Balakrishna\n",
    "\n",
    "Matr num: 6868873\n",
    "\n",
    "Collaborator: Pramod Mahajan \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import shutil\n",
    "import sys\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mnist():\n",
    "    # The code to download the mnist data original came from\n",
    "    # https://cntk.ai/pythondocs/CNTK_103A_MNIST_DataLoader.html\n",
    "    \n",
    "    import gzip\n",
    "    import numpy as np\n",
    "    import os\n",
    "    import struct\n",
    "\n",
    "    try: \n",
    "        from urllib.request import urlretrieve \n",
    "    except ImportError: \n",
    "        from urllib import urlretrieve\n",
    "\n",
    "    def load_data(src, num_samples):\n",
    "        print(\"Downloading \" + src)\n",
    "        gzfname, h = urlretrieve(src, \"./delete.me\")\n",
    "        print(\"Done.\")\n",
    "        try:\n",
    "            with gzip.open(gzfname) as gz:\n",
    "                n = struct.unpack(\"I\", gz.read(4))\n",
    "                # Read magic number.\n",
    "                if n[0] != 0x3080000:\n",
    "                    raise Exception(\"Invalid file: unexpected magic number.\")\n",
    "                # Read number of entries.\n",
    "                n = struct.unpack(\">I\", gz.read(4))[0]\n",
    "                if n != num_samples:\n",
    "                    raise Exception(\n",
    "                        \"Invalid file: expected {0} entries.\".format(num_samples)\n",
    "                    )\n",
    "                crow = struct.unpack(\">I\", gz.read(4))[0]\n",
    "                ccol = struct.unpack(\">I\", gz.read(4))[0]\n",
    "                if crow != 28 or ccol != 28:\n",
    "                    raise Exception(\n",
    "                        \"Invalid file: expected 28 rows/cols per image.\"\n",
    "                    )\n",
    "                # Read data.\n",
    "                res = np.frombuffer(\n",
    "                    gz.read(num_samples * crow * ccol), dtype=np.uint8\n",
    "                )\n",
    "        finally:\n",
    "            os.remove(gzfname)\n",
    "        return res.reshape((num_samples, crow, ccol)) / 256\n",
    "\n",
    "\n",
    "    def load_labels(src, num_samples):\n",
    "        print(\"Downloading \" + src)\n",
    "        gzfname, h = urlretrieve(src, \"./delete.me\")\n",
    "        print(\"Done.\")\n",
    "        try:\n",
    "            with gzip.open(gzfname) as gz:\n",
    "                n = struct.unpack(\"I\", gz.read(4))\n",
    "                # Read magic number.\n",
    "                if n[0] != 0x1080000:\n",
    "                    raise Exception(\"Invalid file: unexpected magic number.\")\n",
    "                # Read number of entries.\n",
    "                n = struct.unpack(\">I\", gz.read(4))\n",
    "                if n[0] != num_samples:\n",
    "                    raise Exception(\n",
    "                        \"Invalid file: expected {0} rows.\".format(num_samples)\n",
    "                    )\n",
    "                # Read labels.\n",
    "                res = np.frombuffer(gz.read(num_samples), dtype=np.uint8)\n",
    "        finally:\n",
    "            os.remove(gzfname)\n",
    "        return res.reshape((num_samples))\n",
    "\n",
    "\n",
    "    def try_download(data_source, label_source, num_samples):\n",
    "        data = load_data(data_source, num_samples)\n",
    "        labels = load_labels(label_source, num_samples)\n",
    "        return data, labels\n",
    "    \n",
    "    \n",
    "    # Not sure why, but yann lecun's website does no longer support \n",
    "    # simple downloader. (e.g. urlretrieve and wget fail, while curl work)\n",
    "    # Since not everyone has linux, use a mirror from uni server.\n",
    "    #     server = 'http://yann.lecun.com/exdb/mnist'\n",
    "    server = 'https://raw.githubusercontent.com/fgnt/mnist/master'\n",
    "    \n",
    "    # URLs for the train image and label data\n",
    "    url_train_image = f'{server}/train-images-idx3-ubyte.gz'\n",
    "    url_train_labels = f'{server}/train-labels-idx1-ubyte.gz'\n",
    "    num_train_samples = 60000\n",
    "\n",
    "    print(\"Downloading train data\")\n",
    "    train_features, train_labels = try_download(url_train_image, url_train_labels, num_train_samples)\n",
    "\n",
    "    # URLs for the test image and label data\n",
    "    url_test_image = f'{server}/t10k-images-idx3-ubyte.gz'\n",
    "    url_test_labels = f'{server}/t10k-labels-idx1-ubyte.gz'\n",
    "    num_test_samples = 10000\n",
    "\n",
    "    print(\"Downloading test data\")\n",
    "    test_features, test_labels = try_download(url_test_image, url_test_labels, num_test_samples)\n",
    "    \n",
    "    return train_features, train_labels, test_features, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading train data\n",
      "Downloading https://raw.githubusercontent.com/fgnt/mnist/master/train-images-idx3-ubyte.gz\n",
      "Done.\n",
      "Downloading https://raw.githubusercontent.com/fgnt/mnist/master/train-labels-idx1-ubyte.gz\n",
      "Done.\n",
      "Downloading test data\n",
      "Downloading https://raw.githubusercontent.com/fgnt/mnist/master/t10k-images-idx3-ubyte.gz\n",
      "Done.\n",
      "Downloading https://raw.githubusercontent.com/fgnt/mnist/master/t10k-labels-idx1-ubyte.gz\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "train_features, train_labels, test_features, test_labels = get_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshape the data\n",
    "train_features_reshape = train_features.reshape(-1, 28*28)\n",
    "test_features_reshape = test_features.reshape(-1, 28*28)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Variable:\n",
    "    def __init__(self, value, operation):\n",
    "        self.value = np.array(value)\n",
    "        self.operation = operation\n",
    "        \n",
    "    \n",
    "#     def backprop(self):\n",
    "#         # We define the backpropagation code later.\n",
    "#         return backprop(self)\n",
    "    \n",
    "class Parameter(Variable):\n",
    "    \"\"\"\n",
    "    This class should be used for Variables that are learnable.\n",
    "    You can later use this class to distinguish learnable variables\n",
    "    from other variables (`isinstance(variable, Parameter)`).\n",
    "    \"\"\"\n",
    "    def __init__(self, value):\n",
    "        super().__init__(value, operation=None)\n",
    "        self.gradient = np.zeros_like(self.value)\n",
    "        \n",
    "class Input(Variable):\n",
    "    \"\"\"\n",
    "    This class should be used as wrapper for inputs that are not learnable.\n",
    "    \"\"\"\n",
    "    def __init__(self, value):\n",
    "        super().__init__(value, operation=None)\n",
    "        \n",
    "\n",
    "class Layer:\n",
    "    \"\"\" Abstract layer class.  \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.parameters = []\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\" to be implemented in the subclasses\n",
    "            implement the forward transformation in each layer\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "\n",
    "        \n",
    "    def add_param(self, values):\n",
    "        \"\"\" All trainable parameters of the layer are appended to the parameters attribute\n",
    "        \"\"\"\n",
    "        param = Parameter(values)\n",
    "        self.parameters.append(param)\n",
    "        return param\n",
    "    \n",
    "    def update_parameters(self, optimizer):\n",
    "        \"\"\" called after each backpropogation\n",
    "        param optimizer: here it is the SGD optimizer\n",
    "        \"\"\"\n",
    "        for param in self.parameters:\n",
    "            optimizer.update(param)\n",
    "        \n",
    "\n",
    "class AffineLayer(Layer):\n",
    "    \"\"\" Layer class that performs a linear transformation on the data in a MLP\n",
    "    \"\"\"\n",
    "    def __init__(self, in_units, out_units):\n",
    "        \"\"\" Initialize weights and biases here. Dimensions based on in_units and out_units\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        small_value = 0.01\n",
    "        weight_vals = np.random.uniform(\n",
    "                    size=[in_units, out_units],\n",
    "                    low=-small_value,\n",
    "                    high=small_value\n",
    "                    )\n",
    "        self.W = self.add_param(weight_vals)\n",
    "        self.b = self.add_param(np.zeros(shape=out_units))\n",
    "        \n",
    "    def forward(self, X):\n",
    "        def backward(D):\n",
    "            #print(self.__class__.__name__)\n",
    "            \n",
    "            self.W.gradient = self.W.gradient + X.T @ D  \n",
    "                                  ## todo to check for correctness (is += required)\n",
    "            self.b.gradient = self.b.gradient + np.sum(D,axis=0) \n",
    "           \n",
    "            return D @ self.W.value.T\n",
    "       \n",
    "       \n",
    "        return X @ self.W.value + self.b.value, backward\n",
    "    \n",
    "    \n",
    "class Sequential(Layer):\n",
    "    \"\"\" Models the multi layer perceptron. layers can only be cascaded one after the other.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, *layers):\n",
    "        \"\"\"  args *layers: pass multiple cascaded layers of the MLP as arguments to this class constructor\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        for layer in layers:\n",
    "            self.parameters.extend(layer.parameters) \n",
    "        \n",
    "    def forward(self, X):\n",
    "        backprops = []\n",
    "        op = X\n",
    "        for layer in self.layers:\n",
    "            op, backprop = layer.forward(op)\n",
    "            backprops.append(backprop)\n",
    "\n",
    "        def backward(D):\n",
    "            for backprop in reversed(backprops):\n",
    "                D = backprop(D)\n",
    "            return D\n",
    "        return op , backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Layer):\n",
    "    def forward(self, X):\n",
    "        mask = X > 0\n",
    "        return X * mask, lambda D: D * mask\n",
    "        \n",
    "    \n",
    "    \n",
    "class Sigmoid(Layer):\n",
    "    def forward(self, X):\n",
    "        op = 1/(1+np.exp(-X))\n",
    "        \n",
    "        def backward(D):\n",
    "            \"\"\" The gradient at output is computed with respect to output of the last affine layer\n",
    "            hence the backward just propogates the previous derivative.\n",
    "            \"\"\"\n",
    "            return D #* op * (1 - op)\n",
    "        \n",
    "        return op, backward\n",
    "    \n",
    "class SGDOptimizer():\n",
    "    def __init__(self, lr=0.1):\n",
    "        self.lr = lr\n",
    "\n",
    "    def update(self, param):\n",
    "        param.value = param.value - self.lr * param.gradient \n",
    "        param.gradient.fill(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(predictions, targets, epsilon=1e-11):\n",
    "    \"\"\"\n",
    "    Computes cross entropy between targets (encoded as one-hot vectors)\n",
    "    and predictions. \n",
    "    Input: predictions ndarray\n",
    "           targets  ndarray        \n",
    "    Returns: scalar Cross Entropy loss\n",
    "    \"\"\"\n",
    "    predictions = np.clip(predictions, epsilon, 1. - epsilon)\n",
    "    N = predictions.shape[0]\n",
    "    ce = -np.sum(targets*np.log(predictions))/N\n",
    "    return ce , predictions - targets   #todo verify derivative of ce loss\n",
    "\n",
    "    \n",
    "\n",
    "def one_hot_encoder(x_label):\n",
    "    \"\"\" returns one hot encoded arrays for the input data labels\n",
    "    \"\"\"\n",
    "    rows = x_label.shape[0]\n",
    "    oh_x = np.zeros((rows, 10))\n",
    "    for i in range(rows):\n",
    "        oh_x[i][x_label[i]] = 1\n",
    "    \n",
    "    return oh_x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class DigitLearner():\n",
    "    \"\"\" Class to train the MLP. \n",
    "    \"\"\"\n",
    "    def __init__(self, model, loss_fn, optimizer):\n",
    "        self.model = model\n",
    "        self.loss = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "    def fit_batch(self,X,Y):\n",
    "        \"\"\" Performs forward and backward operation on one batch of training samples\n",
    "            param X : A batch of training data\n",
    "            param Y : A corresponding batch of training labels\n",
    "        \"\"\"\n",
    "        Y_, backward = self.model.forward(X)\n",
    "        L , D = self.loss(Y_, Y)          \n",
    "                                \n",
    "        backward(D)\n",
    "        self.model.update_parameters(self.optimizer)\n",
    "        return L\n",
    "  \n",
    "    def fit(self, X, Y, epochs, bs):\n",
    "        \"\"\" Fits the entire training data in batches\n",
    "            param X : entire training data\n",
    "            param Y : entire training labels\n",
    "            param bs : batch size for SGD\n",
    "            return : an array of training losses of each epoch \n",
    "        \"\"\"\n",
    "        losses = []\n",
    "        \n",
    "     \n",
    "        for epoch in range(epochs):\n",
    "            p = np.random.permutation(len(X))\n",
    "            L = 0\n",
    "           \n",
    "            for i in range(0, len(X), bs):\n",
    "                X_batch = X[p[i:i + bs]]\n",
    "                Y_batch = Y[p[i:i + bs]]\n",
    "                L += self.fit_batch(X_batch, Y_batch)\n",
    "                  \n",
    "            losses.append(L)\n",
    "\n",
    "        \n",
    "        print(\"Training complete\")\n",
    "        return losses\n",
    "    \n",
    "    def predict(self, xtest):\n",
    "        \"\"\" predict the output given an input batch of samples\n",
    "        \"\"\"\n",
    "        ypred, _ = self.model.forward(xtest)\n",
    "        return ypred.argmax(axis=-1)\n",
    "    \n",
    "    def accuracy(y_pred, y_test):\n",
    "        return np.sum(y_pred == y_test)/y_pred.shape[0]\n",
    "    \n",
    "    def dump_params(self,filename):\n",
    "        \"\"\" writes the trained weights to a json file\n",
    "        \"\"\"\n",
    "        with open(filename, 'w') as fp:\n",
    "            json.dump([p.value.tolist() for p in self.model.parameters],fp)\n",
    "            print(\"weights written to file\")\n",
    "            \n",
    "    def load_params(self, filename):\n",
    "        \"\"\" loads weights from a json file and initializes the ANN with the loaded weights\n",
    "        \"\"\"\n",
    "        with open(filename) as fp:\n",
    "            loaded_params = json.load(fp)\n",
    "        for p, p_value in zip(self.model.parameters, loaded_params):\n",
    "            p.value = p_value\n",
    "        print(\"Loaded given weights!!\")\n",
    "            \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new empty model here to load weights\n",
    "\n",
    "hidden_neurons = 512\n",
    "\n",
    "# create an ANN model with the same architecture as the earlier trained model.\n",
    "mt_model = DigitLearner(\n",
    "    Sequential(\n",
    "        AffineLayer(784, hidden_neurons), \n",
    "        ReLU(), \n",
    "        AffineLayer(hidden_neurons, 10),\n",
    "        Sigmoid()        \n",
    "    ), \n",
    "    cross_entropy, \n",
    "    SGDOptimizer()\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded given weights!!\n"
     ]
    }
   ],
   "source": [
    "# load weights from a saved json file in a new model\n",
    "mt_model.load_params(\"1lyr_512_0.008_99_17_ep_wt.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction accuracy on test dataset is: 99.17 %\n"
     ]
    }
   ],
   "source": [
    "# use the new empty model with weights loaded from json to make prediction\n",
    "tacc = DigitLearner.accuracy(mt_model.predict(test_features_reshape), test_labels)\n",
    "print(\"The prediction accuracy on test dataset is:\", tacc*100,\"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
